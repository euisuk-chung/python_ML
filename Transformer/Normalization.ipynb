{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1,2,3],\n",
    "                  [4,5,6],\n",
    "                  [7,8,9]], dtype=torch.float32)\n",
    "\n",
    "# X = torch.unsqueeze(X, 0)\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고\n",
    "PyTorch에서 dim은 텐서의 차원을 나타내며, 다음과 같이 이해할 수 있습니다:\n",
    "1. dim=0: 첫 번째 차원, 일반적으로 배치(batch) 차원 또는 행(row)을 나타냅니다.\n",
    "2. dim=1: 두 번째 차원, 보통 열(column)을 나타냅니다.\n",
    "3. dim=2: 세 번째 차원, 3D 텐서에서는 깊이(depth)를 나타냅니다.\n",
    "\n",
    "예를 들어, 2D 텐서 X에 대해:\n",
    "```python\n",
    "X = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "```\n",
    "- torch.sum(X, dim=0)은 열 방향으로 합을 계산합니다: [12, 15, 18]\n",
    "- torch.sum(X, dim=1)은 행 방향으로 합을 계산합니다: [6, 15, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12., 15., 18.])\n",
      "tensor([ 6., 15., 24.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(X, dim=0))\n",
    "print(torch.sum(X, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization 결과:\n",
      "tensor([[-1.2247, -1.2247, -1.2247],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 1.2247,  1.2247,  1.2247]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bn = nn.BatchNorm1d(3)  # 3은 특성(feature) 수\n",
    "output_bn = bn(X)\n",
    "print(\"Batch Normalization 결과:\")\n",
    "print(output_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5., 6.])\n",
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(X, dim=0))\n",
    "print(torch.std(X, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization 결과:\n",
      "tensor([[-1.2247, -1.2247, -1.2247],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 1.2247,  1.2247,  1.2247]])\n"
     ]
    }
   ],
   "source": [
    "def batch_norm(X, eps=1e-5):\n",
    "    mean = X.mean(dim=0, keepdim=True)\n",
    "    var = X.var(dim=0, unbiased=False, keepdim=True)\n",
    "    X_norm = (X - mean) / torch.sqrt(var + eps)\n",
    "    return X_norm\n",
    "\n",
    "print(\"Batch Normalization 결과:\")\n",
    "print(batch_norm(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Normalization 결과:\n",
      "tensor([[-1.2247,  0.0000,  1.2247],\n",
      "        [-1.2247,  0.0000,  1.2247],\n",
      "        [-1.2247,  0.0000,  1.2247]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = nn.LayerNorm(3)  # 3은 정규화할 마지막 차원의 크기\n",
    "output_ln = ln(X)\n",
    "print(\"Layer Normalization 결과:\")\n",
    "print(output_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Normalization 결과:\n",
      "tensor([[-1.2247,  0.0000,  1.2247],\n",
      "        [-1.2247,  0.0000,  1.2247],\n",
      "        [-1.2247,  0.0000,  1.2247]])\n"
     ]
    }
   ],
   "source": [
    "def layer_norm(X, eps=1e-5):\n",
    "    mean = X.mean(dim=-1, keepdim=True)\n",
    "    var = X.var(dim=-1, unbiased=False, keepdim=True)\n",
    "    X_norm = (X - mean) / torch.sqrt(var + eps)\n",
    "    return X_norm\n",
    "\n",
    "print(\"Layer Normalization 결과:\")\n",
    "print(layer_norm(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS Normalization 결과:\n",
      "tensor([[0.4629, 0.9258, 1.3887],\n",
      "        [0.7895, 0.9869, 1.1843],\n",
      "        [0.8705, 0.9948, 1.1192]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n",
    "        x_norm = x / rms\n",
    "        return self.scale * x_norm\n",
    "\n",
    "rms_norm = RMSNorm(3)  # 3은 특성(feature) 수\n",
    "output_rms = rms_norm(X)\n",
    "print(\"RMS Normalization 결과:\")\n",
    "print(output_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS Normalization 결과:\n",
      "tensor([[0.4629, 0.9258, 1.3887],\n",
      "        [0.7895, 0.9869, 1.1843],\n",
      "        [0.8705, 0.9948, 1.1192]])\n"
     ]
    }
   ],
   "source": [
    "def rms_norm(X, eps=1e-8):\n",
    "    rms = torch.sqrt(torch.mean(X**2, dim=-1, keepdim=True) + eps)\n",
    "    X_norm = X / rms\n",
    "    return X_norm\n",
    "\n",
    "print(\"RMS Normalization 결과:\")\n",
    "print(rms_norm(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.],\n",
       "          [ 3.,  4.]],\n",
       "\n",
       "         [[ 5.,  6.],\n",
       "          [ 7.,  8.]],\n",
       "\n",
       "         [[ 9., 10.],\n",
       "          [11., 12.]],\n",
       "\n",
       "         [[13., 14.],\n",
       "          [15., 16.]]],\n",
       "\n",
       "\n",
       "        [[[17., 18.],\n",
       "          [19., 20.]],\n",
       "\n",
       "         [[21., 22.],\n",
       "          [23., 24.]],\n",
       "\n",
       "         [[25., 26.],\n",
       "          [27., 28.]],\n",
       "\n",
       "         [[29., 30.],\n",
       "          [31., 32.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 (2개의 샘플, 각 2x2 크기, 4 채널)\n",
    "X = torch.tensor([[\n",
    "                  [[1, 2], [3, 4]],\n",
    "                  [[5, 6], [7, 8]],\n",
    "                  [[9, 10], [11, 12]],\n",
    "                  [[13, 14], [15, 16]]\n",
    "                  ],\n",
    "                  [\n",
    "                  [[17, 18], [19, 20]],\n",
    "                  [[21, 22], [23, 24]],\n",
    "                  [[25, 26], [27, 28]],\n",
    "                  [[29, 30], [31, 32]]\n",
    "                  ]], dtype=torch.float32)\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance Normalization 결과 (PyTorch):\n",
      "tensor([[[[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]]],\n",
      "\n",
      "\n",
      "        [[[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instance Normalization은 주로 2D나 3D 데이터에 사용되지만, \n",
    "# InstanceNorm2d 적용 (4은 채널 수)\n",
    "in_norm = nn.InstanceNorm2d(4, affine=True)\n",
    "output_in = in_norm(X)\n",
    "print(\"Instance Normalization 결과 (PyTorch):\")\n",
    "print(output_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance Normalization 결과 (수동 구현):\n",
      "tensor([[[[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]]],\n",
      "\n",
      "\n",
      "        [[[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]],\n",
      "\n",
      "         [[-1.3416, -0.4472],\n",
      "          [ 0.4472,  1.3416]]]])\n"
     ]
    }
   ],
   "source": [
    "def instance_norm(X, eps=1e-5):\n",
    "    mean = X.mean(dim=(2, 3), keepdim=True)\n",
    "    var = X.var(dim=(2, 3), unbiased=False, keepdim=True)\n",
    "    X_norm = (X - mean) / torch.sqrt(var + eps)\n",
    "    return X_norm\n",
    "\n",
    "print(\"Instance Normalization 결과 (수동 구현):\")\n",
    "print(instance_norm(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch GroupNorm 결과:\n",
      "tensor([[[[-1.5275, -1.0911],\n",
      "          [-0.6547, -0.2182]],\n",
      "\n",
      "         [[ 0.2182,  0.6547],\n",
      "          [ 1.0911,  1.5275]],\n",
      "\n",
      "         [[-1.5275, -1.0911],\n",
      "          [-0.6547, -0.2182]],\n",
      "\n",
      "         [[ 0.2182,  0.6547],\n",
      "          [ 1.0911,  1.5275]]],\n",
      "\n",
      "\n",
      "        [[[-1.5275, -1.0911],\n",
      "          [-0.6547, -0.2182]],\n",
      "\n",
      "         [[ 0.2182,  0.6547],\n",
      "          [ 1.0911,  1.5275]],\n",
      "\n",
      "         [[-1.5275, -1.0911],\n",
      "          [-0.6547, -0.2182]],\n",
      "\n",
      "         [[ 0.2182,  0.6547],\n",
      "          [ 1.0911,  1.5275]]]], grad_fn=<NativeGroupNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def group_norm_pytorch(X, num_groups=2):  # 그룹 수를 2으로 변경\n",
    "    # num_groups: 그룹의 수\n",
    "    # num_channels: 채널의 수 (X의 두 번째 차원)\n",
    "    num_channels = X.shape[1]\n",
    "    gn = nn.GroupNorm(num_groups, num_channels)\n",
    "    return gn(X)\n",
    "\n",
    "# PyTorch GroupNorm 적용\n",
    "gn_pytorch = group_norm_pytorch(X)\n",
    "print(\"\\nPyTorch GroupNorm 결과:\")\n",
    "print(gn_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "수동 구현 GroupNorm 결과:\n",
      "tensor([[[[-1.5275, -1.0911],\n",
      "          [-0.6547, -0.2182]],\n",
      "\n",
      "         [[ 0.2182,  0.6547],\n",
      "          [ 1.0911,  1.5275]],\n",
      "\n",
      "         [[-1.5275, -1.0911],\n",
      "          [-0.6547, -0.2182]],\n",
      "\n",
      "         [[ 0.2182,  0.6547],\n",
      "          [ 1.0911,  1.5275]]],\n",
      "\n",
      "\n",
      "        [[[-1.5275, -1.0911],\n",
      "          [-0.6547, -0.2182]],\n",
      "\n",
      "         [[ 0.2182,  0.6547],\n",
      "          [ 1.0911,  1.5275]],\n",
      "\n",
      "         [[-1.5275, -1.0911],\n",
      "          [-0.6547, -0.2182]],\n",
      "\n",
      "         [[ 0.2182,  0.6547],\n",
      "          [ 1.0911,  1.5275]]]])\n"
     ]
    }
   ],
   "source": [
    "# 수동 구현 GroupNorm\n",
    "def group_norm_manual(X, num_groups=2, eps=1e-5):\n",
    "    N, C, H, W = X.shape\n",
    "    G = num_groups\n",
    "    \n",
    "    # 그룹별로 텐서 재구성\n",
    "    X = X.view(N, G, C // G, H, W)\n",
    "    \n",
    "    # 평균과 분산 계산\n",
    "    mean = X.mean(dim=(2, 3, 4), keepdim=True)\n",
    "    var = X.var(dim=(2, 3, 4), keepdim=True, unbiased=False)\n",
    "    \n",
    "    # 정규화\n",
    "    X = (X - mean) / torch.sqrt(var + eps)\n",
    "    \n",
    "    # 원래 shape으로 복원\n",
    "    return X.view(N, C, H, W)\n",
    "\n",
    "# 수동 구현 GroupNorm 적용\n",
    "gn_manual = group_norm_manual(X)\n",
    "print(\"\\n수동 구현 GroupNorm 결과:\")\n",
    "print(gn_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ML-39zzOMfa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
